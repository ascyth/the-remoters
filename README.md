# Tech Report:


	There are 1000 data points representing The Guardian articles, 640 data points representing Reddit posts and 900 data points representing the New York Times articles. Hence, all together we have 2,540 data points. Our data is cleaned and free of any duplicates. We believe this data is sufficient to perform our data as it gives us plenty of input to feed an AI/ML algorithm. 
  
	Our identifying attributes include the ARTICLE_ID, URL, DATE, PLAIN_TEXT and SOURCE. The ARTICLE_ID is a unique ID that distinguishes the articles from each other, retrieved from the source (the Guardian, Reddit and the New York Times). 
  
	The Guardian data is from the Guardian API, using “remote work” as a query. The Guardian is a reliable British newspaper and the Guardian API has accurate representations of their articles. The Reddit data from the Reddit API, specifically pertaining to posts from the “experienceddevs,” “programming”, and “cscareerquestions” subreddits that had one or more of the keywords “remote work,” “work from home” or “WFH.” In order to ensure that the posts included in the dataset were reputable, we only included posts that received more than 200 upvotes. The New York Times data was collected using their API, focusing on the query “remote work.” The API has an accurate, reliable representation of the articles relevant to the topic. For our three different sources, we aimed to make our data collection and cleaning processes as homogeneous as possible and tried to account for key differences in the sources. The sample has 100 rows and was generated by randomly selecting articles from the three sources.
  
We implemented the same cleaning strategy for all data regardless of the source in order to ensure consistency among the data. We removed any non-word syntax and special characters such as “\n,” “<.*?>” among others. We also removed any unnecessary spaces and made sure all text was in unidecode. By the end of our cleaning process, all the PLAIN_TEXT only included words and necessary punctuation.

Before we checked the cleanliness of our data, we checked the data structure and verified that all data was well-formed and had values for the 5 attributes. We then checked for missing values by using “df.isnull().sum().” We verified that our data had no missing data. We checked for duplicates using “df.duplicated().sum().” We also checked that PLAIN_TEXT was clean and that all special characters were removed. We did not need to throw any data away and did not encounter any major problems with our data sets.

We have been pleasantly surprised with the sheer amount of articles and posts regarding remote work and are confident that we have sufficient data to feed our AI/ML model in order to characterize attitudes toward remote work in different online communities. The APIs made the retrieval process much easier. Moreover, using the same cleaning method for all data simplified the cleaning process. One challenge was combining all the data together and ensuring that there were no ID duplicates and the IDs were retrieved from the sources and not randomly generated by us. Our next step is to feed all our input into an AI/ML model in order to begin analyzing the data.
